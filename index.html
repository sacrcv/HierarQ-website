<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding">
 
  <meta name="keywords" content="Video understanding, task-awareness, hierarchical Q-Former">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>


<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-1 publication-title">HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding </h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- put shhereen link -->
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Shehreen Azad</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://vibhav-vineet.github.io" target="_blank">Vibhav Vineet</a><sup>2</sup>,</span>
                <span class="author-block">
                    <a href="https://www.crcv.ucf.edu/person/rawat/" target="_blank">Yogesh Singh Rawat</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Center for Research in Computer Vision, University of Central Florida; <sup>2</sup> Microsoft Research.
                  <span class="author-block">
                       <br> <h1 class="title is-4"><font color="#B03A2E"><b>CVPR 2025</b></font></h1>
                  </span>
                    </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                       <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.08585" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> 

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2503.08585" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                </span>

                  <span class="link-block">
                    <a href="https://sacrcv.github.io/HierarQ-website/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
    

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-justified"></h2>
      <h2 class="subtitle has-text-justified">
        <span style="font-weight:bold;">HierarQ  </span>
        is a task-aware Hierarchical Q-Former based framework that processes videos auto-regressively
        without frame sampling. This setup preserves efficient processing while allowing HierarQ to maintain a task-focused,
        human-like cognitive approach, dynamically emphasizing
        relevant details based on task requirements.</h2>
      <img src="./static/images/teaser.jpg" height="100%" width="100%"/> 
      <!-- <h2 class="subtitle has-text-centered"> -->
        <h2 class="hero-body has-text-centered"></h2>
        Effectiveness of HierarQ in capturing task-relevant information. HierarQ adaptively focuses on task-relevant video segments, achieving a task-aware, comprehensive understanding. Here, color-coded frames are shown to demonstrate how entity-focused information complements the broader prompt-relevant context, enhancing overall video relevance and understanding.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite advancements in multimodal large language models (MLLMs), current approaches struggle in medium-to-long video understanding due to frame and context length limitations. As a result, these models often depend on frame sampling, which risks missing key information over time and lack task-specific relevance. To address these challenges, we introduce <b>HierarQ</b>, a task-aware hierarchical Q-Former based framework that sequentially processes frames to bypass the need for frame sampling, while avoiding LLM's context length limitations. We introduce a lightweight two-stream language-guided feature modulator to incorporate task awareness in video understanding, with the entity stream capturing frame-level object information within a short context and the scene stream identifying their broader interactions over longer period of time. Each stream is supported by dedicated memory banks which enables our proposed <b>Hierar</b>chical <b>Q</b>uerying transformer (HierarQ) to effectively capture short and long-term context. Extensive evaluations on <b>10</b> video benchmarks across video understanding, question answering, and captioning tasks demonstrate HierarQ's state-of-the-art performance across most datasets, proving its robustness and efficiency for comprehensive video analysis.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper architecture -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">Architecture</h2>
    </div> </div>
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered has-text-justified"></div>
      <img src="./static/images/arch-final.jpg" width="80%"/> 
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- <div class="column is-half"></div> -->
        <h2 class="hero-body has-text-justified"></h2>
        (Left) Overview of our framework that sequentially processes video frames, modulating task-relevant entity and scene features with a two-stream feature modulator. The proposed HierarQ (Hierarchical Q-Former) with dedicated memory banks integrates these features, producing a refined understanding that is passed to an LLM for the final response. (Right) HierarQ models the hierarchical relationship between then Entity-level Q-Former and Scene-level Q-Former, using dedicated memory banks to integrate short-term details with long-term context for enhanced video understanding.
      </h2>
      </div>
    </div>
  </div>
    </section>

<!-- Paper Quantitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">Quantitative Results</h2>
      </div>
    </div>

      <!-- <h2 class="subtitle has-text-centered"> -->
      <h2 class="hero-body has-text-justified"></h2>
      Medium to long video understanding performance on LVU (Left) and Breakfast, COIN (Right).
      </h2>
      <img src="./static/images/lvu-brf.jpg" width="100%"/> 
      
      <h2 class="hero-body has-text-justified"></h2>
       Long video question answering on MovieChat-1K (Left). Short video question answering performance on MSRVTT-QA (denoted by MSR-QA), MSVD-QA and ActivityNet-QA (denoted by ANet-QA) (Right).
      </h2>
      <img src="./static/images/msr-mc.jpg" width="100%"/> 
      <!-- <h2 class="subtitle has-text-centered"> -->
      
    <!-- <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered"></div>
          <div id="results-carousel" class="carousel results-carousel">
           <div class="item">  -->
            <!-- Your image here -->
            <!-- <img src="static/images/lvu.png" alt="MY ALT TEXT" width="100%"/>
            <h2 class="subtitle has-text-centered">
              Medium to long video understanding on LVU dataset. 
            </h2>
          </div>
          <div class="item"> --> 
            <!-- Your image here -->
            <!-- <img src="static/images/brfc.png" alt="MY ALT TEXT" width="70%"/>
            <h2 class="subtitle has-text-centered">
              Medium to long video understanding on Breakfast and COIN datasets. 
            </h2>
          </div>
          <div class="item"> -->
            <!-- Your image here -->
            <!-- <img src="static/images/mc.png" alt="MY ALT TEXT" width="100%"/>
            <h2 class="subtitle has-text-centered">
              Long video question answering on MovieChat-1k in Global (G) and Breakpoint (B) mode.
           </h2>
         </div>
         <div class="item"> -->
          <!-- Your image here -->
          <!-- <img src="static/images/msr.png" alt="MY ALT TEXT" width="95%"/>
          <h2 class="subtitle has-text-centered">
            Short video question answering on MSRVTT-QA (denoted by MSR-QA), MSVD-QA and ActivityNet-QA (denoted by ANet-QA)
         </h2>
       </div>
       <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/vcap.png" alt="MY ALT TEXT" width="100%"/>
        <h2 class="subtitle has-text-centered">
          Video captioning on MSRVTT, MSVD and YouCook2 datasets.
       </h2>
     </div> -->
    </div>
  </div>
  </div>
  <!-- <div class="hero-body">
    <div class="container">
    <h2 class="content has-text-justified">
      Performance comparison of HierarQ against other models across several video understanding tasks.
      </h2>
    </div>
  </div> -->
  </section>
</section>



<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item"> 
        <!-- Your image here -->
        <img src="static/images/qual1.jpg" alt="MY ALT TEXT"/>
        <!-- <h2 class="subtitle has-text-centered">
          First image description.
        </h2> -->
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual2.jpg" alt="MY ALT TEXT"/>
        <!-- <h2 class="subtitle has-text-centered">
          Second image description.
        </h2> -->
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qual3.jpg" alt="MY ALT TEXT"/>
        <!-- <h2 class="subtitle has-text-centered">
         Third image description.
       </h2> -->
     </div>
  </div>
</div>
</div>
<div class="hero-body">
  <div class="container">
  <h2 class="content has-text-justified">
    Qualitative analysis of long-video question answering on MovieChat-1k. Here, HierarQ adaptively focuses on task-relevant
video segments, achieving a task-aware, comprehensive understanding. Color-coded frames are shown to demonstrate how entity-focused
information complements the broader prompt-relevant context, enhancing overall video relevance and understanding.
    </h2>
  </div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{azad2025hierarq,
        title={HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding},
        author={Azad, Shehreen and Vineet, Vibhav and Rawat, Yogesh Singh},
        journal={arXiv preprint arXiv:2503.08585},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://nerfies.github.io" target="_blank">Nerfies project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
